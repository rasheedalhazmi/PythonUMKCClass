{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pyspark-DataFrame-Sample.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohLGTKOGDE7F"
      },
      "source": [
        "Installing the dependencies for spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbfdKQlmhSIG"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.3-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcLKpV1bhjlC"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop3.2\""
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B80QS_ndhz8X",
        "outputId": "20043c8d-c0bd-4da0-eeeb-ee61a2ea4831"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "!pip install pyspark==3.0.3"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.0.3 in /usr/local/lib/python3.7/dist-packages (3.0.3)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark==3.0.3) (0.10.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhUA6388DLXf"
      },
      "source": [
        "Creating a spark session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzFyBGR9ilyu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "77baeae5-046a-437f-eeca-2d3fe7e9b97c"
      },
      "source": [
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-382852aa6cd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.ui.port'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'4050'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[*]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    341\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 343\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    344\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-4-382852aa6cd1>:2 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY5sBVLmDO6e"
      },
      "source": [
        "Loading the spark UI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbN_PFTSh9LO",
        "outputId": "8b577ad0-268d-4513-8bc2-6ef5c977d803"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-13 02:56:16--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.202.168.65, 54.237.133.81, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.202.168.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  64.0MB/s    in 0.2s    \n",
            "\n",
            "2021-09-13 02:56:16 (64.0 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqqWg86BDSuL"
      },
      "source": [
        "Go to the link generated in the output to visit the spark UI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_s0n0nAiYeW",
        "outputId": "5484e8c8-48c0-4ac4-c937-17ab0cafa01c"
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"tunnels\":[{\"name\":\"command_line\",\"uri\":\"/api/tunnels/command_line\",\"public_url\":\"https://217d-35-245-217-213.ngrok.io\",\"proto\":\"https\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":1,\"gauge\":0,\"rate1\":2.1622517425246317e-22,\"rate5\":3.5113178295070544e-7,\"rate15\":0.0000524751278903042,\"p50\":30129163841,\"p90\":30129163841,\"p95\":30129163841,\"p99\":30129163841},\"http\":{\"count\":1,\"rate1\":1.3114719758582564e-22,\"rate5\":3.177171758754793e-7,\"rate15\":0.00005075478857035456,\"p50\":121095636,\"p90\":121095636,\"p95\":121095636,\"p99\":121095636}}},{\"name\":\"command_line (http)\",\"uri\":\"/api/tunnels/command_line%20%28http%29\",\"public_url\":\"http://217d-35-245-217-213.ngrok.io\",\"proto\":\"http\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":64,\"gauge\":0,\"rate1\":2.635818750209139e-15,\"rate5\":0.00011521886636229623,\"rate15\":0.004820331653532405,\"p50\":38051690.5,\"p90\":10939248970,\"p95\":30253750061.5,\"p99\":38990183413},\"http\":{\"count\":142,\"rate1\":5.572238608407707e-15,\"rate5\":0.0002560658353154566,\"rate15\":0.010706903103894503,\"p50\":5347643.5,\"p90\":24280970.600000005,\"p95\":32111884.549999993,\"p99\":999683494.3999907}}}],\"uri\":\"/api/tunnels\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns_D3dBeDcdg"
      },
      "source": [
        "Creating the spark dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s18OtSgUyvFn"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructField, StructType, StringType, LongType"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbVVNSjU3dw_"
      },
      "source": [
        "schema=StructType(\n",
        "    [\n",
        "     StructField(name='city', dataType=StringType(), nullable=True),\n",
        "     StructField(name='country',dataType=StringType(), nullable=True),\n",
        "     StructField(name='counts',dataType=LongType(),nullable=False),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRnlNrD656HK"
      },
      "source": [
        "rows=[\n",
        "      Row('Los Angeles','United States',3),\n",
        "      Row('New York', 'United States', 1),\n",
        "      Row('London','United Kingdom', 1),\n",
        "]"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YmZQyRI6c1h"
      },
      "source": [
        "parallelizeRows=spark.sparkContext.parallelize(rows)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s84v7TjO6kYx"
      },
      "source": [
        "df=spark.createDataFrame(parallelizeRows,schema)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCiO2O286uFl",
        "outputId": "fdd2e9b6-7c89-486b-a90e-38b02d3907a1"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+------+\n",
            "|       city|       country|counts|\n",
            "+-----------+--------------+------+\n",
            "|Los Angeles| United States|     3|\n",
            "|   New York| United States|     1|\n",
            "|     London|United Kingdom|     1|\n",
            "+-----------+--------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgMcy6BJ7C9-"
      },
      "source": [
        "Create a dataframe from different types of files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "RTxLOrSk66uW",
        "outputId": "ac969761-f520-443c-f68f-fce565bb4fed"
      },
      "source": [
        "#Dataframe from csv\n",
        "df=spark.read.csv('yourfile.csv',inferSchema=True,header=True)\n",
        "#Dataframe from json\n",
        "df_json=spark.read.json('yourfile.json')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b258137c248d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Dataframe from csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yourfile.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#Dataframe from json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yourfile.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/yourfile.csv;"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ScVYZ6a7jqv"
      },
      "source": [
        "Printing the schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0ZoaFZG7lrJ",
        "outputId": "07f46eae-b0bb-452b-f102-2696c866a633"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- city: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- counts: long (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bliYg6xe70NT"
      },
      "source": [
        "Manupilating Columns-: Columns in spark are similar to columns in pandas. We can select, transform and remove columns with the use of expressions. We cannot manupilate a column outside the context of a dataframe, therefore we need spark transformations within a dataframe to modify a column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxJZJQAy8TAa"
      },
      "source": [
        "import pyspark.sql.functions as F"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Owp_TedJ8ajY",
        "outputId": "b08fffd2-2081-4302-b457-e356558e1ffe"
      },
      "source": [
        "df.select('country').show(1)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|      country|\n",
            "+-------------+\n",
            "|United States|\n",
            "+-------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5edXHC48gY1",
        "outputId": "0e90c160-ffc0-480e-ab15-6939bcf1585c"
      },
      "source": [
        "df.select(F.col('country')).show(1)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|      country|\n",
            "+-------------+\n",
            "|United States|\n",
            "+-------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKQUZ07P8tG7",
        "outputId": "8e720d1e-761a-4ffc-f481-1c6031b67c22"
      },
      "source": [
        "df.select('country','city').show(1)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----------+\n",
            "|      country|       city|\n",
            "+-------------+-----------+\n",
            "|United States|Los Angeles|\n",
            "+-------------+-----------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDhu5xG99Z5g"
      },
      "source": [
        "Change the column name in the expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZr0QHWN9UOP",
        "outputId": "56d4d9b8-0674-43b1-c936-30aa59abc591"
      },
      "source": [
        "df.select(F.expr('country as destination')).show(2)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|  destination|\n",
            "+-------------+\n",
            "|United States|\n",
            "|United States|\n",
            "+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSpj93PO9mC7",
        "outputId": "c34db03c-b053-4f5b-a4a1-81cfb008d8a7"
      },
      "source": [
        "df.select(F.expr('country as destination').alias('country')).show(2)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|      country|\n",
            "+-------------+\n",
            "|United States|\n",
            "|United States|\n",
            "+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWW6RQJF9_MK",
        "outputId": "48f5d5b7-ff2f-4c4b-9c4d-4900f1992969"
      },
      "source": [
        "new_df=df.selectExpr('country as new_country','country')\n",
        "new_df.show()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------+\n",
            "|   new_country|       country|\n",
            "+--------------+--------------+\n",
            "| United States| United States|\n",
            "| United States| United States|\n",
            "|United Kingdom|United Kingdom|\n",
            "+--------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-4pYCFt-TQl",
        "outputId": "a99f294e-f747-40de-a7d9-79825f15fb5d"
      },
      "source": [
        "new_df2=df.selectExpr('avg(counts)','count(distinct(country))')\n",
        "new_df2.show()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----------------------+\n",
            "|       avg(counts)|count(DISTINCT country)|\n",
            "+------------------+-----------------------+\n",
            "|1.6666666666666667|                      2|\n",
            "+------------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG87myRC-vCS"
      },
      "source": [
        "Passing Explicit Value with literals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIAGVK-N-sG3",
        "outputId": "db0d921c-60ee-4fe2-95cf-6411e4420b84"
      },
      "source": [
        "df.select(F.expr('*'),F.lit(1).alias('One')).show()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+------+---+\n",
            "|       city|       country|counts|One|\n",
            "+-----------+--------------+------+---+\n",
            "|Los Angeles| United States|     3|  1|\n",
            "|   New York| United States|     1|  1|\n",
            "|     London|United Kingdom|     1|  1|\n",
            "+-----------+--------------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOfZzkMV_DQh"
      },
      "source": [
        "Adding a column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFQ9xdUk_Eww",
        "outputId": "5d747d3a-dba6-4509-e455-5918786deb6f"
      },
      "source": [
        "df=df.withColumn('One',F.lit(1))\n",
        "df.show()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+------+---+\n",
            "|       city|       country|counts|One|\n",
            "+-----------+--------------+------+---+\n",
            "|Los Angeles| United States|     3|  1|\n",
            "|   New York| United States|     1|  1|\n",
            "|     London|United Kingdom|     1|  1|\n",
            "+-----------+--------------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MjKNSU7_Z6r"
      },
      "source": [
        "Renaming Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heLpkgra_bxI",
        "outputId": "c194e0ee-d2d2-433f-effd-58734517e3cf"
      },
      "source": [
        "df=df.withColumnRenamed('one','ONE')\n",
        "df.columns"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['city', 'country', 'counts', 'ONE']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42GTVZSz_sYE"
      },
      "source": [
        "Removing Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK0v_WZ6_qq_",
        "outputId": "4afda677-22b1-4e23-8d08-dae223405aaa"
      },
      "source": [
        "df=df.drop('ONE')\n",
        "df.columns"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['city', 'country', 'counts']"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mInAMwt4__Ut"
      },
      "source": [
        "DataFrame filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqOImK8QACLe",
        "outputId": "1cdd863e-6683-4e8c-e4fd-b06f19672a53"
      },
      "source": [
        "df.filter(F.col('counts')<2).show(1)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------+------+\n",
            "|    city|      country|counts|\n",
            "+--------+-------------+------+\n",
            "|New York|United States|     1|\n",
            "+--------+-------------+------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aZUFcU1AXBY"
      },
      "source": [
        "If we need to perform multiple filters at the same time, we can chain the operations together as Spark will perform all the operations at the same time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXRFFuelAi5c",
        "outputId": "0109a262-f34c-4b00-9e3d-2026e51f73c0"
      },
      "source": [
        "df.where(F.col('counts')<=1).where(F.col('country')!='United States').show()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+------+\n",
            "|  city|       country|counts|\n",
            "+------+--------------+------+\n",
            "|London|United Kingdom|     1|\n",
            "+------+--------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbkSrcH7A6TS"
      },
      "source": [
        "Getting the distinct rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5CeKs1SA5y8",
        "outputId": "4f8825e6-20e1-491b-f101-9b8466341aa0"
      },
      "source": [
        "df.select('city').distinct().count()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKVRRQ-8BEWI"
      },
      "source": [
        "Getting random samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O4yNMfzBHCj",
        "outputId": "3c7c56c4-af00-4710-a2d5-c226d630c264"
      },
      "source": [
        "df.sample(withReplacement=False,fraction=1.0,seed=5).count()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QsbRZqbBQbG"
      },
      "source": [
        "Random splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVL0eQyIBTOv",
        "outputId": "ebbad0c8-eff5-434c-e1ab-6a5948005865"
      },
      "source": [
        "df2=df.randomSplit([0.67,0.33],seed=5)\n",
        "print(df.count())\n",
        "print(df2[0].count())\n",
        "print(df2[1].count())"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "2\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm7BRHgkBsVf"
      },
      "source": [
        "Dataframe are immutable,\n",
        "So to append items in the dataframes create new dataframe and union with the old dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKl6x1uGB5zE"
      },
      "source": [
        "rows=[\n",
        "      Row('Berlin','Germany',2),\n",
        "      Row('Bucharest','Romnia',2),\n",
        "]\n",
        "parallelizeRows=spark.sparkContext.parallelize(rows)\n",
        "df2=spark.createDataFrame(rows,schema)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8OiBo0wCTy3"
      },
      "source": [
        "df3=df.union(df2)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuJu9xNrCYbn",
        "outputId": "9e450018-c9e3-46cf-9a71-1d8b328f39d5"
      },
      "source": [
        "df3.createOrReplaceTempView('new_df')\n",
        "df3.show()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+------+\n",
            "|       city|       country|counts|\n",
            "+-----------+--------------+------+\n",
            "|Los Angeles| United States|     3|\n",
            "|   New York| United States|     1|\n",
            "|     London|United Kingdom|     1|\n",
            "|     Berlin|       Germany|     2|\n",
            "|  Bucharest|        Romnia|     2|\n",
            "+-----------+--------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybsv00krCrBT"
      },
      "source": [
        "Limiting what we want from the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf8XKRYGC6zf",
        "outputId": "4d17807b-1dae-471c-ddb4-65d59e06d701"
      },
      "source": [
        "df.limit(3).show()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+------+\n",
            "|       city|       country|counts|\n",
            "+-----------+--------------+------+\n",
            "|Los Angeles| United States|     3|\n",
            "|   New York| United States|     1|\n",
            "|     London|United Kingdom|     1|\n",
            "+-----------+--------------+------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}